{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jgesc-05/clase-inteligencia-artificial/blob/main/Cuaderno_7_Dividir_los_Datos_de_Prueba_y_Entrenamiento_y_el_de_Balanceo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color=\"red\">Cuaderno 7: Dividir los Datos de Prueba y Entrenamiento y el de Balanceo\n",
        "En el ámbito del machine learning, una parte fundamental del proceso es la división de los datos en conjuntos de entrenamiento y prueba, así como el balanceo de clases cuando se trabaja con conjuntos de datos desbalanceados. En este Sesion, exploraremos cómo dividir los datos para asegurar que el modelo entrene y se evalúe correctamente, así como las técnicas que se pueden usar para balancear clases en problemas de clasificación.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## <font color=\"red\">7.1 Técnicas para Dividir los Datos en Conjuntos de Entrenamiento y Prueba\n",
        "El primer paso en el entrenamiento de un modelo es dividir los datos en dos conjuntos principales: entrenamiento y prueba. El conjunto de entrenamiento es utilizado para ajustar el modelo, mientras que el conjunto de prueba se usa para evaluar su rendimiento y generalización.\n",
        "### <font color=\"blue\">7.1.1 Método de División Común: train_test_split\n",
        "La librería scikit-learn proporciona la función train_test_split para dividir fácilmente los datos en conjuntos de entrenamiento y prueba. El principal parámetro es el tamaño de la prueba, que usualmente se establece en un 20-30% de los datos totales. También es recomendable utilizar el parámetro stratify para garantizar que la división mantenga la misma proporción de clases en ambos conjuntos, especialmente en problemas de clasificación.\n",
        "### Uso de train_test_split con Stratify\n",
        "Cuando se trabaja con problemas de clasificación, es importante que la distribución de las clases se mantenga consistente entre el conjunto de entrenamiento y el conjunto de prueba. Esto se logra utilizando el parámetro stratify.\n",
        "\n"
      ],
      "metadata": {
        "id": "oUV8crqLJbeB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Cargar el conjunto de datos de ejemplo\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Dividir los datos en entrenamiento (80%) y prueba (20%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Verificación de la distribución de clases en los conjuntos de entrenamiento y prueba\n",
        "print(\"Distribución de clases en entrenamiento:\", dict(zip(*np.unique(y_train, return_counts=True))))\n",
        "print(\"Distribución de clases en prueba:\", dict(zip(*np.unique(y_test, return_counts=True))))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "B-hHM9ZbJzRB",
        "outputId": "4c37887f-6d38-40f7-e0ef-fad2fcbf6c73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distribución de clases en entrenamiento: {0: 40, 1: 40, 2: 40}\n",
            "Distribución de clases en prueba: {0: 10, 1: 10, 2: 10}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* random_state=42: Este parámetro asegura que la división de los datos sea reproducible. Es una buena práctica fijar un valor para los fines de la experimentación controlada.\n",
        "* stratify=y: Este parámetro asegura que la proporción de las clases en el conjunto de entrenamiento y prueba sea igual a la proporción en el conjunto original.\n",
        "### Consideraciones Adicionales en la División de Datos\n",
        "* Proporción de datos de prueba: Un porcentaje comúnmente utilizado para los datos de prueba es entre el 20% y 30%. Sin embargo, el porcentaje exacto puede depender del tamaño total del conjunto de datos y el contexto del problema.\n",
        "* Randomización: Para evitar cualquier sesgo en la división de los datos, es importante aleatorizar la asignación de muestras a los conjuntos de entrenamiento y prueba. El parámetro random_state controla la aleatorización para garantizar reproducibilidad.\n",
        "\n",
        "\n",
        "---\n",
        "## <font color=\"red\">7.2 Estrategias para Balancear los Datos\n",
        "En muchos problemas de clasificación, especialmente en el caso de clases desbalanceadas, el modelo puede aprender a predecir principalmente la clase mayoritaria, ignorando la clase minoritaria. Esto puede generar un rendimiento subóptimo, ya que el modelo no generaliza bien para las clases menos representadas. Para abordar este problema, existen varias estrategias de balanceo de clases.\n",
        "### <font color=\"blue\">7.2.1 Sobremuestreo (Oversampling)\n",
        "El sobremuestreo implica duplicar las muestras de la clase minoritaria para igualar su número con las clases mayoritarias. Esto ayuda a que el modelo aprenda mejor sobre las características de la clase minoritaria.\n",
        "### Uso de SMOTE para Sobremuestreo\n",
        "Una técnica avanzada de sobremuestreo es SMOTE (Synthetic Minority Over-sampling Technique), que genera nuevas muestras sintéticas para la clase minoritaria en lugar de simplemente duplicar las existentes.\n",
        "\n"
      ],
      "metadata": {
        "id": "_7Nf5OsKJ1rY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Crear un conjunto de datos desbalanceado\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, weights=[0.9, 0.1], random_state=42)\n",
        "\n",
        "# Aplicar SMOTE para generar muestras sintéticas de la clase minoritaria\n",
        "smote = SMOTE(random_state=42)\n",
        "X_res, y_res = smote.fit_resample(X, y)\n",
        "\n",
        "# Verificar la nueva distribución de clases\n",
        "print(\"Distribución de clases después de SMOTE:\", dict(zip(*np.unique(y_res, return_counts=True))))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RcW5yA0IKaYQ",
        "outputId": "d0010c53-2780-4e6b-80cf-5c2637992ae1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distribución de clases después de SMOTE: {0: 897, 1: 897}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* SMOTE crea nuevas instancias para la clase minoritaria interpolando entre ejemplos cercanos, en lugar de simplemente duplicar los datos.\n",
        "\n",
        "### <font color=\"blue\">7.2.2 Submuestreo (Undersampling)\n",
        "El submuestreo implica reducir el número de muestras de la clase mayoritaria para igualar las clases. Esto puede perder información importante de la clase mayoritaria, pero puede ser útil en situaciones donde la clase mayoritaria tiene un exceso de datos.\n"
      ],
      "metadata": {
        "id": "6143faKeKcE_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "# Aplicar submuestreo aleatorio a los datos desbalanceados\n",
        "undersampler = RandomUnderSampler(random_state=42)\n",
        "X_res, y_res = undersampler.fit_resample(X, y)\n",
        "\n",
        "# Verificar la distribución de clases después del submuestreo\n",
        "print(\"Distribución de clases después de submuestreo:\", dict(zip(*np.unique(y_res, return_counts=True))))\n"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rsx0uh3PKiy_",
        "outputId": "6a61efa7-1968-4238-b112-0ae01f14ea70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distribución de clases después de submuestreo: {0: 103, 1: 103}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color=\"blue\">7.2.3 Importancia del Balance de Clases\n",
        "El balance de clases es crucial en muchos problemas de clasificación, ya que un modelo entrenado en datos desbalanceados puede tener una alta precisión, pero solo porque está prediciendo la clase mayoritaria con alta frecuencia. Si no se abordan estos desequilibrios, el modelo podría tener un rendimiento deficiente para las clases menos representadas.\n",
        "\n",
        "El uso de técnicas de balanceo mejora la capacidad del modelo para aprender de las clases minoritarias, resultando en un modelo más equilibrado y con un rendimiento más generalizado.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## <font color=\"red\">7.3 Técnicas Combinadas de Sobremuestreo y Submuestreo\n",
        "En algunos casos, una combinación de sobremuestreo y submuestreo puede ser más eficaz que usar cualquiera de las dos técnicas por separado. Esto se logra aplicando SMOTE para aumentar la clase minoritaria y luego aplicando un submuestreo en la clase mayoritaria para reducir el desbalance de forma más eficiente.\n"
      ],
      "metadata": {
        "id": "TeubvyBIKkX3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.combine import SMOTEENN\n",
        "\n",
        "# Aplicar SMOTE y submuestreo utilizando SMOTEENN\n",
        "smote_enn = SMOTEENN(random_state=42)\n",
        "X_res, y_res = smote_enn.fit_resample(X, y)\n",
        "\n",
        "# Verificar la distribución de clases después de la combinación\n",
        "print(\"Distribución de clases después de SMOTE + Submuestreo:\", dict(zip(*np.unique(y_res, return_counts=True))))\n",
        "-"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNbqUxDRKvUv",
        "outputId": "65a764a7-da4d-4e61-bacd-e477fa03879e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distribución de clases después de SMOTE + Submuestreo: {0: 671, 1: 896}\n"
          ]
        }
      ]
    }
  ]
}